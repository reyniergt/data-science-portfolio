{
  
    
        "post0": {
            "title": "Digits recognition",
            "content": "MNIST (&quot;Modified National Institute of Standards and Technology&quot;) is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike. . In this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. We’ve curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms to learn first-hand what works well and how techniques compare. . Dataset exploration . First, let&#39;s load and explore the training dataset . import pandas as pd import numpy as np import os for dirname, _, filenames in os.walk(&#39;./digits/&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . ./digits/three_1.png ./digits/train.csv ./digits/six_1.png ./digits/nine_1.png . TRAIN_CSV = &#39;./digits/train.csv&#39; df = pd.read_csv(TRAIN_CSV) df.describe() . label pixel0 pixel1 pixel2 pixel3 pixel4 pixel5 pixel6 pixel7 pixel8 ... pixel774 pixel775 pixel776 pixel777 pixel778 pixel779 pixel780 pixel781 pixel782 pixel783 . count 42000.000000 | 42000.0 | 42000.0 | 42000.0 | 42000.0 | 42000.0 | 42000.0 | 42000.0 | 42000.0 | 42000.0 | ... | 42000.000000 | 42000.000000 | 42000.000000 | 42000.00000 | 42000.000000 | 42000.000000 | 42000.0 | 42000.0 | 42000.0 | 42000.0 | . mean 4.456643 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.219286 | 0.117095 | 0.059024 | 0.02019 | 0.017238 | 0.002857 | 0.0 | 0.0 | 0.0 | 0.0 | . std 2.887730 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 6.312890 | 4.633819 | 3.274488 | 1.75987 | 1.894498 | 0.414264 | 0.0 | 0.0 | 0.0 | 0.0 | . min 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.000000 | 0.000000 | 0.000000 | 0.00000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | . 25% 2.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.000000 | 0.000000 | 0.000000 | 0.00000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | . 50% 4.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.000000 | 0.000000 | 0.000000 | 0.00000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | . 75% 7.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.000000 | 0.000000 | 0.000000 | 0.00000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | . max 9.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 254.000000 | 254.000000 | 253.000000 | 253.00000 | 254.000000 | 62.000000 | 0.0 | 0.0 | 0.0 | 0.0 | . 8 rows × 785 columns . Our dataset has 42000 rows and 785 columns. . The first column label is the digit, while the rest 784 columns pixeli represents the value of the i_th pixel . import seaborn as sn sn.set() . sn.countplot(x=&#39;label&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a475cfe10&gt; . We have around 4000 examples of every digit. Lets split our dataset in two parts for training and testing . from sklearn.model_selection import train_test_split train_features, test_features, train_labels, test_labels = train_test_split(df.drop(columns=[&#39;label&#39;]), df[&#39;label&#39;], test_size=0.2, random_state=42) . train_labels . 34941 6 24433 5 24432 3 8832 4 30291 7 .. 6265 9 11284 9 38158 2 860 6 15795 0 Name: label, Length: 33600, dtype: int64 . sn.countplot(x=&#39;label&#39;, data=pd.DataFrame(train_labels)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a47463b90&gt; . Model training . Now that we split our dataset between train and test, lets chose a classification model . from sklearn.tree import DecisionTreeClassifier tree = DecisionTreeClassifier() tree.fit(train_features, train_labels) . DecisionTreeClassifier() . tree.predict(df.drop(columns=[&#39;label&#39;]).head()) . array([1, 0, 1, 4, 0]) . from sklearn.ensemble import RandomForestClassifier forest = RandomForestClassifier(n_estimators=10) forest.fit(train_features, train_labels) . RandomForestClassifier(n_estimators=10) . forest.predict(df.drop(columns=[&#39;label&#39;]).head()) . array([1, 0, 1, 4, 0]) . from sklearn.neural_network import MLPClassifier nn = MLPClassifier(hidden_layer_sizes=(60, 30, 10), random_state=1) nn.fit(train_features, train_labels) . MLPClassifier(hidden_layer_sizes=(60, 30, 10), random_state=1) . nn.predict(df.drop(columns=[&#39;label&#39;]).head()) . array([1, 0, 1, 4, 0]) . Model evaluation . Let&#39;s evaluate our models using accuracy ad recall. . from sklearn.metrics import precision_score, recall_score, accuracy_score . eval_df = pd.DataFrame(columns=[&#39;model&#39;, &#39;accuracy&#39;, &#39;recall&#39;]) for name, model in {&#39;decision tree&#39;: tree, &#39;random forest&#39;: forest, &#39;neural network&#39;: nn}.items(): eval_df = eval_df.append({&#39;model&#39;:name, &#39;accuracy&#39;: accuracy_score(test_labels, model.predict(test_features)), &#39;recall&#39;:recall_score(test_labels, model.predict(test_features), average=&#39;micro&#39;)}, ignore_index=True) . eval_df . model accuracy recall . 0 decision tree | 0.853810 | 0.853810 | . 1 random forest | 0.938452 | 0.938452 | . 2 neural network | 0.954167 | 0.954167 | . eval_df.plot(x=&#39;model&#39;, y=&#39;accuracy&#39;, kind=&#39;bar&#39;) eval_df.plot(x=&#39;model&#39;, y=&#39;recall&#39;, kind=&#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a45530550&gt; . As we can see, the neural network has a slightly better accuracy and recall than the decision trees and random forest. So, we will use our neural network model in order to classify. .",
            "url": "https://reyniergt.github.io/data-science-portfolio/2020/08/09/_06_10_Digits_recognition.html",
            "relUrl": "/2020/08/09/_06_10_Digits_recognition.html",
            "date": " • Aug 9, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Titanic",
            "content": "The Challenge . The sinking of the Titanic is one of the most infamous shipwrecks in history. . On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. . While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. . Our goal is to be able to predict who survived. . Data Dictionary . Variable Definition Key . survival | Survival | 0 = No, 1 = Yes | . pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd | . sex | Sex | male, female | . Age | Age in years | | . sibsp | # of siblings / spouses aboard the Titanic | | . parch | # of parents / children aboard the Titanic | | . ticket | Ticket number | | . fare | Passenger fare | | . cabin | Cabin number | | . embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton | . . Variable Notes pclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower . age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5 . sibsp: The dataset defines family relations in this way... Sibling = brother, sister, stepbrother, stepsister Spouse = husband, wife (mistresses and fiancés were ignored) . parch: The dataset defines family relations in this way... Parent = mother, father Child = daughter, son, stepdaughter, stepson Some children travelled only with a nanny, therefore parch=0 for them. . # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here&#39;s several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only &quot;../input/&quot; directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won&#39;t be saved outside of the current session . /kaggle/input/titanic/train.csv /kaggle/input/titanic/gender_submission.csv /kaggle/input/titanic/test.csv . Lets load the dataset using pandas . TRAIN_CSV = &#39;/kaggle/input/titanic/train.csv&#39; . train_df = pd.read_csv(TRAIN_CSV) . %matplotlib inline import matplotlib.pyplot as plt import seaborn as sns sns.set() . Lets explore the test dataset . train_df.shape . (891, 12) . The train dataset has 12 columns and 891 rows Lets see the first 5 rows . train_df.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . Lets understand every column starting with PassengerId . train_df[&#39;PassengerId&#39;].describe() . count 891.000000 mean 446.000000 std 257.353842 min 1.000000 25% 223.500000 50% 446.000000 75% 668.500000 max 891.000000 Name: PassengerId, dtype: float64 . Lets explore the remaining columns . train_df.describe() . PassengerId Survived Pclass Age SibSp Parch Fare . count 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 | . std 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 | . min 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | . 25% 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 | . 50% 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 | . 75% 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 | . max 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | . plt.hist(x=&#39;Age&#39;, data=train_df) plt.xlabel(&#39;Age&#39;) plt.ylabel(&#39;Age Frequency&#39;) . /opt/conda/lib/python3.7/site-packages/numpy/lib/histograms.py:839: RuntimeWarning: invalid value encountered in greater_equal keep = (tmp_a &gt;= first_edge) /opt/conda/lib/python3.7/site-packages/numpy/lib/histograms.py:840: RuntimeWarning: invalid value encountered in less_equal keep &amp;= (tmp_a &lt;= last_edge) . Text(0, 0.5, &#39;Age Frequency&#39;) . The age is distributed between 0 and 80 years, with predominance of those between 20 and 40. . Summarizing the data: . Survived: 0 (No) / 1 (Yes) | Pclass: 1 (First class)/ 2 (Second class) / 3 (Third class) | Name: String | Sex: male / female | Age: Number between 0 and 80, predominant between 20 and 40. | SibSp: Number of Siblings/spouses aboard the Titanic (present values between 0 to 8) | Parch: of parents/children aboard the Titanic (present values between 0 to 6) | ticket: Ticket number | fare: Passenger fare | cabin: Cabin number | embarked: Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton | . Lets find missing data . train_df[train_df.isnull().any(axis=1)] . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . 5 6 | 0 | 3 | Moran, Mr. James | male | NaN | 0 | 0 | 330877 | 8.4583 | NaN | Q | . 7 8 | 0 | 3 | Palsson, Master. Gosta Leonard | male | 2.0 | 3 | 1 | 349909 | 21.0750 | NaN | S | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 884 885 | 0 | 3 | Sutehall, Mr. Henry Jr | male | 25.0 | 0 | 0 | SOTON/OQ 392076 | 7.0500 | NaN | S | . 885 886 | 0 | 3 | Rice, Mrs. William (Margaret Norton) | female | 39.0 | 0 | 5 | 382652 | 29.1250 | NaN | Q | . 886 887 | 0 | 2 | Montvila, Rev. Juozas | male | 27.0 | 0 | 0 | 211536 | 13.0000 | NaN | S | . 888 889 | 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | female | NaN | 1 | 2 | W./C. 6607 | 23.4500 | NaN | S | . 890 891 | 0 | 3 | Dooley, Mr. Patrick | male | 32.0 | 0 | 0 | 370376 | 7.7500 | NaN | Q | . 708 rows × 12 columns . There are 708 rows with null values, lets take it into consideration Now, lets start exploring the survival rate . sns.countplot(x=&#39;Survived&#39;, data=train_df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f348d52ee10&gt; . Unfortunatly, more than 500 people died vs 300 survivors, which means a random person is more likely to died instead of survive. Lets explore now the relationship between survival rate and our colums, starting with Pclass. . sns.countplot(x=&#39;Pclass&#39;, hue=&#39;Survived&#39;, data=train_df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f348d412a50&gt; . As we can see, a person in first class is more likely to survive, while a person in third class is more likely to perish Second class doesnt show a big diference. This could be explained by people of higher social classes having priority to board the boats, or by the distance from every class to the boats (Remember first class were closer to the top of the ship, while lower classes were closer to the bottom). . Lets go with sex. . sns.countplot(x=&#39;Sex&#39;, hue=&#39;Survived&#39;, data=train_df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f348d37c150&gt; . Males are more likely to die, while females are more likely to survive. It could be explained by women having priority to board the boats. . agegrouped = train_df[[&#39;Survived&#39;, &#39;Age&#39;]] agegrouped[&#39;group&#39;] = agegrouped[&#39;Age&#39;].floordiv(10) agegrouped . /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . Survived Age group . 0 0 | 22.0 | 2.0 | . 1 1 | 38.0 | 3.0 | . 2 1 | 26.0 | 2.0 | . 3 1 | 35.0 | 3.0 | . 4 0 | 35.0 | 3.0 | . ... ... | ... | ... | . 886 0 | 27.0 | 2.0 | . 887 1 | 19.0 | 1.0 | . 888 0 | NaN | NaN | . 889 1 | 26.0 | 2.0 | . 890 0 | 32.0 | 3.0 | . 891 rows × 3 columns . sns.countplot(x=&#39;group&#39;, hue=&#39;Survived&#39;,data=agegrouped) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f348d3753d0&gt; . It seems children 0-10 years are more likely to survive, while elders 60+ didnt make it. The range 10-60 has a more probability to perish, especially the 20-30 which has a strong tendency to die. This could be explained by the childrens having priority to board the botes, while young ones were left last. . Lets explore the relationship between travel companions and survival probabilities . train_df[&#39;travel_companions&#39;] = train_df[&#39;SibSp&#39;]+train_df[&#39;Parch&#39;] sns.countplot(x=&#39;travel_companions&#39;, hue=&#39;Survived&#39;,data=train_df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f348d28f5d0&gt; . It seems like small families (1 to 3 companions had bigger survival probabilities than the rest). Remember women and children had also bigger survival chances. . sns.catplot(x=&#39;Survived&#39;, y=&#39;Fare&#39;,data=train_df) . &lt;seaborn.axisgrid.FacetGrid at 0x7f348d4192d0&gt; . It seems there isnt a strong relationship between fare and survival, except for those who paid the most (we can infer those who traveled in first class) . We would think there shouldnt be any relationship between survival and name, ticket number and embarcation port. But just in case lets explore it. . sns.countplot(hue=&#39;Survived&#39;, x=&#39;Embarked&#39;, data=train_df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f348d185ed0&gt; . It looks like those embarked in S(Southampton) and Q(Queenstown) had more probability to die, while those embarqued on C(Cherbourg) had a minimal improve in its survival probabilities. Lets explore why could be that. . sns.countplot(hue=&#39;Pclass&#39;, x=&#39;Embarked&#39;, data=train_df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f348d100910&gt; . As we can see, people who embarked on C(Cherbourg) were mainly first class, while the rest were mainly third. Lets remember those in first class had more probability to survive. . This exploration show us the main signals we could use for predicting who survived: . Sex | Pclass | Age | Travel companions (SibSp and Parch) | Embarked | . in order to start training, lets split our dataset in two (train and eval) . In first place we need to preprocess our dataset . Lets start by treating missing data . FEATURES = [&#39;Age&#39;, &#39;Sex&#39;, &#39;Pclass&#39;, &#39;travel_companions&#39;, &#39;Embarked&#39;] LABEL = &#39;Survived&#39; df = train_df[FEATURES] df[LABEL]=train_df[LABEL] df . /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy after removing the cwd from sys.path. . Age Sex Pclass travel_companions Embarked Survived . 0 22.0 | male | 3 | 1 | S | 0 | . 1 38.0 | female | 1 | 1 | C | 1 | . 2 26.0 | female | 3 | 0 | S | 1 | . 3 35.0 | female | 1 | 1 | S | 1 | . 4 35.0 | male | 3 | 0 | S | 0 | . ... ... | ... | ... | ... | ... | ... | . 886 27.0 | male | 2 | 0 | S | 0 | . 887 19.0 | female | 1 | 0 | S | 1 | . 888 NaN | female | 3 | 3 | S | 0 | . 889 26.0 | male | 1 | 0 | C | 1 | . 890 32.0 | male | 3 | 0 | Q | 0 | . 891 rows × 6 columns . df.isnull().sum() . Age 177 Sex 0 Pclass 0 travel_companions 0 Embarked 2 Survived 0 dtype: int64 . Of the 891 rows there are 177 that hasnt Age values, and 2 that hasnt Embarked values. We can remove the two that hasnt Embarked port, given the number is unsignificant. . However, the number of rows with null Age values is not small (177 of 891) Lets use the median for this. . from sklearn.impute import SimpleImputer imputer = SimpleImputer() df[&#39;Age&#39;] = pd.DataFrame(imputer.fit_transform(df[&#39;Age&#39;].values.reshape(-1,1))[:,0]) . /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy This is separate from the ipykernel package so we can avoid doing imports until . df.dropna(subset=[&#39;Embarked&#39;], inplace=True) . /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . df.isnull().sum() . Age 0 Sex 0 Pclass 0 travel_companions 0 Embarked 0 Survived 0 dtype: int64 . Now that we havent any missing values left, lets encoded our not numerical values. Remember the the model only accepts numerical values. . from sklearn.preprocessing import LabelEncoder, StandardScaler . encoder = LabelEncoder() df.Sex = encoder.fit_transform(df.Sex) . /opt/conda/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . df.Embarked = encoder.fit_transform(df.Embarked) . /opt/conda/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . df . Age Sex Pclass travel_companions Embarked Survived . 0 22.000000 | 1 | 3 | 1 | 2 | 0 | . 1 38.000000 | 0 | 1 | 1 | 0 | 1 | . 2 26.000000 | 0 | 3 | 0 | 2 | 1 | . 3 35.000000 | 0 | 1 | 1 | 2 | 1 | . 4 35.000000 | 1 | 3 | 0 | 2 | 0 | . ... ... | ... | ... | ... | ... | ... | . 886 27.000000 | 1 | 2 | 0 | 2 | 0 | . 887 19.000000 | 0 | 1 | 0 | 2 | 1 | . 888 29.699118 | 0 | 3 | 3 | 2 | 0 | . 889 26.000000 | 1 | 1 | 0 | 0 | 1 | . 890 32.000000 | 1 | 3 | 0 | 1 | 0 | . 889 rows × 6 columns . Discretize the age, in order to use a finite number of values based on the age range . df.Age = df.Age.div(10).astype(&#39;int32&#39;) . /opt/conda/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = value . df . Age Sex Pclass travel_companions Embarked Survived . 0 2 | 1 | 3 | 1 | 2 | 0 | . 1 3 | 0 | 1 | 1 | 0 | 1 | . 2 2 | 0 | 3 | 0 | 2 | 1 | . 3 3 | 0 | 1 | 1 | 2 | 1 | . 4 3 | 1 | 3 | 0 | 2 | 0 | . ... ... | ... | ... | ... | ... | ... | . 886 2 | 1 | 2 | 0 | 2 | 0 | . 887 1 | 0 | 1 | 0 | 2 | 1 | . 888 2 | 0 | 3 | 3 | 2 | 0 | . 889 2 | 1 | 1 | 0 | 0 | 1 | . 890 3 | 1 | 3 | 0 | 1 | 0 | . 889 rows × 6 columns . Our dataset seems to be ready for training. In order to do this lets start by spliting our data in two datasets, for training and testing. . features_df = df[FEATURES] labels_df = df[LABEL] . from sklearn.model_selection import train_test_split train_features, test_features, train_labels, test_labels = train_test_split(features_df, labels_df, test_size=0.2, random_state=42) . train_features.shape . (711, 5) . test_features.shape . (178, 5) . Our train dataset has 711 rows vs 178 in our test dataset Lets start by choosing a DecisionTree classifier . from sklearn import tree model = tree.DecisionTreeClassifier() model.fit(train_features, train_labels) . DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=&#39;deprecated&#39;, random_state=None, splitter=&#39;best&#39;) . Our model is trained and ready for predicting. Lets try some . model.predict(train_features.head(5)) . array([1, 0, 0, 0, 0]) . Now we need to evaluate our model. As this is a classification problem, we need to use two main metrics: precision and recall. . from sklearn.metrics import precision_score, recall_score, accuracy_score . Compute the precision . The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. . The best value is 1 and the worst value is 0. . accuracy_score(train_labels, model.predict(train_features)) . 0.8734177215189873 . accuracy_score(test_labels, model.predict(test_features)) . 0.8089887640449438 . precision_score(train_labels, model.predict(train_features)) . 0.9371980676328503 . precision_score(test_labels, model.predict(test_features)) . 0.7777777777777778 . Compute the recall . The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. . The best value is 1 and the worst value is 0. . recall_score(train_labels, model.predict(train_features)) . 0.7158671586715867 . recall_score(test_labels, model.predict(test_features)) . 0.7101449275362319 . Our DecisionTree model shows a precision of 77% and a recall of 71% in the test dataset. Lets try to improve it using a different model . from sklearn.ensemble import RandomForestClassifier forest = RandomForestClassifier(n_estimators=10) forest.fit(train_features, train_labels) forest.predict(train_features.head(5)) . array([1, 0, 0, 0, 0]) . Now, lets evaluate the forest classifier using the same metrics . accuracy_score(train_labels, forest.predict(train_features)) . 0.869198312236287 . accuracy_score(test_labels, forest.predict(test_features)) . 0.8089887640449438 . precision_score(train_labels, forest.predict(train_features)) . 0.8869565217391304 . precision_score(test_labels, forest.predict(test_features)) . 0.7611940298507462 . recall_score(train_labels, forest.predict(train_features)) . 0.7527675276752768 . recall_score(test_labels, forest.predict(test_features)) . 0.7391304347826086 . There seems to be an improvement of 2% in for both precision and recall in favor of the RandomForest model .",
            "url": "https://reyniergt.github.io/data-science-portfolio/2020/08/09/_05_05_Kaggle_Titanic_competition.html",
            "relUrl": "/2020/08/09/_05_05_Kaggle_Titanic_competition.html",
            "date": " • Aug 9, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://reyniergt.github.io/data-science-portfolio/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://reyniergt.github.io/data-science-portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://reyniergt.github.io/data-science-portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}